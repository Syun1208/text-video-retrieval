{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from sentence-transformers)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting torchvision (from sentence-transformers)\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from sentence-transformers) (1.24.4)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/bf/15/d1b649fc7685d11b806b4546a5438191fb2ad761de70da95ff676189dcec/scikit_learn-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk (from sentence-transformers)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece (from sentence-transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.4.0 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.4.0->sentence-transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting fsspec (from huggingface-hub>=0.4.0->sentence-transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting requests (from huggingface-hub>=0.4.0->sentence-transformers)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.4.0->sentence-transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/c8/6b/6600ac24725c7388255b2f5add93f91e58a5d7efaf4af244fdbcc11a541b/PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Collecting sympy (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch>=1.6.0->sentence-transformers)\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers) (0.38.4)\n",
      "Collecting cmake (from triton==2.0.0->torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/69/5c/961fc973ff91af8985c3290a80d169bb5c0beb083593c8614eb2ae18fab3/cmake-3.27.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading cmake-3.27.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting lit (from triton==2.0.0->torch>=1.6.0->sentence-transformers)\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/1f/5c/374ac3fa3c7ed9a967ad273a5e841897ef6b10aa6aad938ff10717a3e2a3/regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4d/81/9b6ee8bd7faf7ae79afafde28b4e8abbcb897c9aa089d51eb5d0a1f3ffcd/safetensors-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting click (from nltk->sentence-transformers)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->sentence-transformers)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->sentence-transformers)\n",
      "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://files.pythonhosted.org/packages/8b/b3/d7b6ee16358d829ca482c74a96e2b9079bf33f8d7d37d16f8ebb19ddf5a4/Pillow-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading Pillow-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->sentence-transformers)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/de/e2/32c14301bb023986dff527a49325b6259cab4ebb4633f69de54af312fc45/MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.4.0->sentence-transformers)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/cb/e7/5e43745003bf1f90668c7be23fc5952b3a2b9c2558f16749411c18039b36/charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.8.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.3/774.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading cmake-3.27.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sentence-transformers, lit\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=37c61959c342a6ffd51e3f3c81df7821debbc166a32919f2e4f90406b25687ce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_h61hc3n/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93583 sha256=c8b91eda266089c9ae0f64ea55fd29366df3051d0100da101676a084ca28afde\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_h61hc3n/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\n",
      "Successfully built sentence-transformers lit\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, mpmath, lit, cmake, tqdm, threadpoolctl, sympy, scipy, regex, pyyaml, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, MarkupSafe, joblib, fsspec, filelock, click, charset-normalizer, scikit-learn, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, nltk, jinja2, huggingface-hub, transformers, triton, torch, torchvision, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.3 charset-normalizer-3.2.0 click-8.1.6 cmake-3.27.1 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 jinja2-3.1.2 joblib-1.3.2 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nltk-3.8.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-10.0.0 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 safetensors-0.3.2 scikit-learn-1.3.0 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 tqdm-4.66.1 transformers-4.31.0 triton-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client>=1.1.1\n",
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a semantic search for Sci-Fi books! \n",
    "documents = [\n",
    "  { \"name\": \"The Time Machine\", \"description\": \"A man travels through time and witnesses the evolution of humanity.\", \"author\": \"H.G. Wells\", \"year\": 1895 },\n",
    "  { \"name\": \"Ender's Game\", \"description\": \"A young boy is trained to become a military leader in a war against an alien race.\", \"author\": \"Orson Scott Card\", \"year\": 1985 },\n",
    "  { \"name\": \"Brave New World\", \"description\": \"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\", \"author\": \"Aldous Huxley\", \"year\": 1932 },\n",
    "  { \"name\": \"The Hitchhiker's Guide to the Galaxy\", \"description\": \"A comedic science fiction series following the misadventures of an unwitting human and his alien friend.\", \"author\": \"Douglas Adams\", \"year\": 1979 },\n",
    "  { \"name\": \"Dune\", \"description\": \"A desert planet is the site of political intrigue and power struggles.\", \"author\": \"Frank Herbert\", \"year\": 1965 },\n",
    "  { \"name\": \"Foundation\", \"description\": \"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\", \"author\": \"Isaac Asimov\", \"year\": 1951 },\n",
    "  { \"name\": \"Snow Crash\", \"description\": \"A futuristic world where the internet has evolved into a virtual reality metaverse.\", \"author\": \"Neal Stephenson\", \"year\": 1992 },\n",
    "  { \"name\": \"Neuromancer\", \"description\": \"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\", \"author\": \"William Gibson\", \"year\": 1984 },\n",
    "  { \"name\": \"The War of the Worlds\", \"description\": \"A Martian invasion of Earth throws humanity into chaos.\", \"author\": \"H.G. Wells\", \"year\": 1898 },\n",
    "  { \"name\": \"The Hunger Games\", \"description\": \"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\", \"author\": \"Suzanne Collins\", \"year\": 2008 },\n",
    "  { \"name\": \"The Andromeda Strain\", \"description\": \"A deadly virus from outer space threatens to wipe out humanity.\", \"author\": \"Michael Crichton\", \"year\": 1969 },\n",
    "  { \"name\": \"The Left Hand of Darkness\", \"description\": \"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\", \"author\": \"Ursula K. Le Guin\", \"year\": 1969 },\n",
    "  { \"name\": \"The Time Traveler's Wife\", \"description\": \"A love story between a man who involuntarily time travels and the woman he loves.\", \"author\": \"Audrey Niffenegger\", \"year\": 2003 }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create collection to store books\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=\"my_books\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's vectorize descriptions and upload to qdrant\n",
    "\n",
    "qdrant.upload_records(\n",
    "    collection_name=\"my_books\",\n",
    "    records=[\n",
    "        models.Record(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(doc[\"description\"]).tolist(),\n",
    "            payload=doc\n",
    "        ) for idx, doc in enumerate(documents)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'The War of the Worlds', 'description': 'A Martian invasion of Earth throws humanity into chaos.', 'author': 'H.G. Wells', 'year': 1898} score: 0.5265540495038633\n",
      "{'name': 'The Andromeda Strain', 'description': 'A deadly virus from outer space threatens to wipe out humanity.', 'author': 'Michael Crichton', 'year': 1969} score: 0.4260535272664947\n",
      "{'name': \"The Hitchhiker's Guide to the Galaxy\", 'description': 'A comedic science fiction series following the misadventures of an unwitting human and his alien friend.', 'author': 'Douglas Adams', 'year': 1979} score: 0.36173440995681116\n"
     ]
    }
   ],
   "source": [
    "# Let's now search for something\n",
    "\n",
    "hits = qdrant.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=encoder.encode(\"Aliens attack our planet\").tolist(),\n",
    "    limit=3\n",
    ")\n",
    "for hit in hits:\n",
    "  print(hit.payload, \"score:\", hit.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'The Hunger Games', 'description': 'A dystopian society where teenagers are forced to fight to the death in a televised spectacle.', 'author': 'Suzanne Collins', 'year': 2008} score: 0.32551047537293154\n",
      "{'name': \"The Time Traveler's Wife\", 'description': 'A love story between a man who involuntarily time travels and the woman he loves.', 'author': 'Audrey Niffenegger', 'year': 2003} score: 0.13160474631551444\n"
     ]
    }
   ],
   "source": [
    "# Let's now search only for books from 21st century\n",
    "\n",
    "hits = qdrant.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=encoder.encode(\"Tyranic society\").tolist(),\n",
    "    query_filter=models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"year\",\n",
    "                range=models.Range(\n",
    "                    gte=2000\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    limit=3\n",
    ")\n",
    "for hit in hits:\n",
    "  print(hit.payload, \"score:\", hit.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=\"my_books\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size= 512, # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.recreate_collection(\n",
    "    collection_name=\"my_books\",\n",
    "    vectors_config=models.VectorParams(size=512, distance=models.Distance.COSINE),\n",
    "    optimizers_config=models.OptimizersConfigDiff(memmap_threshold=2000),\n",
    "    quantization_config=models.ProductQuantization(\n",
    "        product=models.ProductQuantizationConfig(\n",
    "            compression=models.CompressionRatio.X64,\n",
    "            always_ram=True,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "vectors = np.random.rand(5000, 512)\n",
    "client.upsert(\n",
    "    collection_name=\"my_books\",\n",
    "    points=[\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=vector.tolist(),\n",
    "            payload={\"color\": \"red\", \"rand_number\": idx % 100}\n",
    "        )\n",
    "        for idx, vector in enumerate(vectors)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017362356185913086"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector = np.random.rand(512)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "hits = client.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=query_vector,\n",
    "    limit=10  # Return 5 closest points\n",
    ")\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: autofaiss in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (2.15.8)\n",
      "Requirement already satisfied: fire<0.5.0,>=0.4.0 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (0.4.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.5 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (1.24.4)\n",
      "Requirement already satisfied: pandas<2,>=1.1.5 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (1.5.3)\n",
      "Requirement already satisfied: pyarrow<13,>=6.0.1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (12.0.1)\n",
      "Requirement already satisfied: tqdm<5,>=4.62.3 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2022.1.0 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (2023.6.0)\n",
      "Requirement already satisfied: embedding-reader<2,>=1.5.1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (1.5.1)\n",
      "Requirement already satisfied: faiss-cpu<2,>=1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from autofaiss) (1.7.4)\n",
      "Requirement already satisfied: six in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from fire<0.5.0,>=0.4.0->autofaiss) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from fire<0.5.0,>=0.4.0->autofaiss) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from pandas<2,>=1.1.5->autofaiss) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/toonies/anaconda3/envs/Qqdrant/lib/python3.8/site-packages (from pandas<2,>=1.1.5->autofaiss) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 09:20:06,697 [INFO]: Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2023-08-13 09:20:06,701 [INFO]: Launching the whole pipeline 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,702 [INFO]: Reading total number of vectors and dimension 08/13/2023, 09:20:06\n",
      "100%|██████████| 1/1 [00:00<00:00, 12409.18it/s]\n",
      "2023-08-13 09:20:06,795 [INFO]: There are 2100 embeddings of dim 512\n",
      "2023-08-13 09:20:06,796 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0935 secs\n",
      "2023-08-13 09:20:06,797 [INFO]: \tCompute estimated construction time of the index 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,798 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2023-08-13 09:20:06,799 [INFO]: \t\t-> Add: 0.0 seconds\n",
      "2023-08-13 09:20:06,800 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2023-08-13 09:20:06,802 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0032 secs\n",
      "2023-08-13 09:20:06,802 [INFO]: \tChecking that your have enough memory available to create the index 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,804 [INFO]: 4.8MB of memory will be needed to build the index (more might be used if you have more)\n",
      "2023-08-13 09:20:06,805 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0017 secs\n",
      "2023-08-13 09:20:06,805 [INFO]: \tSelecting most promising index types given data characteristics 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,806 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2023-08-13 09:20:06,806 [INFO]: \tCreating the index 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,807 [INFO]: \t\t-> Instanciate the index HNSW15 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,810 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW15\" in 0.0029 secs\n",
      "2023-08-13 09:20:06,811 [INFO]: \t\t-> Adding the vectors to the index 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:06,812 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n",
      "2023-08-13 09:20:06,813 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.24it/s]\n",
      "2023-08-13 09:20:06,960 [INFO]: \tComputing best hyperparameters for index /home/toonies/AI-Challenge/PLIP-ELASTICSEARCH/knn.index 08/13/2023, 09:20:06\n",
      "2023-08-13 09:20:10,348 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /home/toonies/AI-Challenge/PLIP-ELASTICSEARCH/knn.index\" in 3.3879 secs\n",
      "2023-08-13 09:20:10,349 [INFO]: The best hyperparameters are: efSearch=16383\n",
      "2023-08-13 09:20:10,350 [INFO]: \tCompute fast metrics 08/13/2023, 09:20:10\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "2023-08-13 09:20:16,099 [INFO]: \t>>> Finished \"Compute fast metrics\" in 5.7485 secs\n",
      "2023-08-13 09:20:16,100 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 9.2882 secs\n",
      "2023-08-13 09:20:16,101 [INFO]: {\n",
      "2023-08-13 09:20:16,101 [INFO]: \tindex_key: HNSW15\n",
      "2023-08-13 09:20:16,102 [INFO]: \tindex_param: efSearch=16383\n",
      "2023-08-13 09:20:16,102 [INFO]: \tindex_path: /home/toonies/AI-Challenge/PLIP-ELASTICSEARCH/knn.index\n",
      "2023-08-13 09:20:16,103 [INFO]: \tsize in bytes: 4586230\n",
      "2023-08-13 09:20:16,103 [INFO]: \tavg_search_speed_ms: 2.8434325680000536\n",
      "2023-08-13 09:20:16,104 [INFO]: \t99p_search_speed_ms: 7.2784803499541795\n",
      "2023-08-13 09:20:16,104 [INFO]: \treconstruction error %: 0.0\n",
      "2023-08-13 09:20:16,105 [INFO]: \tnb vectors: 2100\n",
      "2023-08-13 09:20:16,105 [INFO]: \tvectors dimension: 512\n",
      "2023-08-13 09:20:16,105 [INFO]: \tcompression ratio: 0.9377636969798724\n",
      "2023-08-13 09:20:16,106 [INFO]: }\n",
      "2023-08-13 09:20:16,106 [INFO]: \t>>> Finished \"Creating the index\" in 9.2996 secs\n",
      "2023-08-13 09:20:16,108 [INFO]: >>> Finished \"Launching the whole pipeline\" in 9.4060 secs\n"
     ]
    }
   ],
   "source": [
    "from autofaiss import build_index\n",
    "import numpy as np\n",
    "\n",
    "embeddings = np.float32(np.random.rand(2100, 512))\n",
    "index, index_infos = build_index(embeddings, save_on_disk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011642694473266602\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "query = np.float32(np.random.rand(1, 512))\n",
    "_, I = index.search(query, 1)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1075200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.flatten().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qqdrant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
